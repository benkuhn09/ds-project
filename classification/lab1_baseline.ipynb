{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "traf_df = pd.read_csv(\"data/traffic_accidents.csv\")\n",
    "traf_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traf_target = traf_df[\"crash_type\"] #pulling target out before dropping non-numerics\n",
    "\n",
    "traf_features = traf_df.drop(columns=[\"crash_type\"])\n",
    "\n",
    "traf_features = traf_features.dropna(axis=1, how=\"all\") # dropping empty variables col-wise\n",
    "traf_features = traf_features.dropna(axis=0, how=\"any\") #dropping rows with any missing values\n",
    "\n",
    "traf_target = traf_target.loc[traf_features.index] #target and remaining rows aligned\n",
    "\n",
    "traf_features = traf_features.select_dtypes(include=[\"number\"]) #keeping only numeric cols\n",
    "\n",
    "print(traf_features.info())\n",
    "print(\"Features shape:\", traf_features.shape)\n",
    "print(\"Target shape:\", traf_target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test splitting\n",
    "traf_features_train, traf_features_test, traf_target_train, traf_target_test = train_test_split(\n",
    "    traf_features,\n",
    "    traf_target,\n",
    "    test_size=0.3,        #.7 train, .3 test\n",
    "    random_state=42,      \n",
    "    stratify=traf_target       # keeps class proportions similar in train and test\n",
    ")\n",
    "\n",
    "print(traf_features_train.shape, traf_features_test.shape)\n",
    "print(traf_target_train.shape, traf_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval helper, maybe delete later\n",
    "\n",
    "def evaluate_model(model_name, model, features_train, target_train, features_test, target_test):\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions = model.predict(features_test)\n",
    "\n",
    "    accuracy = accuracy_score(target_test, predictions)\n",
    "    precision = precision_score(target_test, predictions, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(target_test, predictions, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    print(f\"--- {model_name} ---\")\n",
    "    print(f\"Accuracy : {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall   : {recall:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    return accuracy, precision, recall, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all traffic results\n",
    "traf_results = {}\n",
    "\n",
    "traf_naive_bayes = GaussianNB()\n",
    "acc, prec, rec, preds = evaluate_model(\n",
    "    \"Naive Bayes (Traffic)\",\n",
    "    traf_naive_bayes,\n",
    "    traf_features_train,\n",
    "    traf_target_train,\n",
    "    traf_features_test,\n",
    "    traf_target_test\n",
    ")\n",
    "traf_results['Naive Bayes'] = {'accuracy': acc, 'precision': prec, 'recall': rec, 'predictions': preds}\n",
    "\n",
    "traf_logistic_regression = LogisticRegression(max_iter=1000)\n",
    "acc, prec, rec, preds = evaluate_model(\n",
    "    \"Logistic Regression (Traffic)\",\n",
    "    traf_logistic_regression,\n",
    "    traf_features_train,\n",
    "    traf_target_train,\n",
    "    traf_features_test,\n",
    "    traf_target_test\n",
    ")\n",
    "traf_results['Logistic Regression'] = {'accuracy': acc, 'precision': prec, 'recall': rec, 'predictions': preds}\n",
    "\n",
    "traf_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "acc, prec, rec, preds = evaluate_model(\n",
    "    \"KNN (Traffic, k=5)\",\n",
    "    traf_knn,\n",
    "    traf_features_train,\n",
    "    traf_target_train,\n",
    "    traf_features_test,\n",
    "    traf_target_test\n",
    ")\n",
    "traf_results['KNN'] = {'accuracy': acc, 'precision': prec, 'recall': rec, 'predictions': preds}\n",
    "\n",
    "traf_decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "acc, prec, rec, preds = evaluate_model(\n",
    "    \"Decision Tree (Traffic)\",\n",
    "    traf_decision_tree,\n",
    "    traf_features_train,\n",
    "    traf_target_train,\n",
    "    traf_features_test,\n",
    "    traf_target_test\n",
    ")\n",
    "traf_results['Decision Tree'] = {'accuracy': acc, 'precision': prec, 'recall': rec, 'predictions': preds}\n",
    "\n",
    "traf_mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=300, random_state=42)\n",
    "acc, prec, rec, preds = evaluate_model(\n",
    "    \"MLP (Traffic, 50 hidden units)\",\n",
    "    traf_mlp,\n",
    "    traf_features_train,\n",
    "    traf_target_train,\n",
    "    traf_features_test,\n",
    "    traf_target_test\n",
    ")\n",
    "traf_results['MLP'] = {'accuracy': acc, 'precision': prec, 'recall': rec, 'predictions': preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison for Traffic Accidents dataset\n",
    "models = list(traf_results.keys())\n",
    "accuracy_scores = [traf_results[m]['accuracy'] for m in models]\n",
    "precision_scores = [traf_results[m]['precision'] for m in models]\n",
    "recall_scores = [traf_results[m]['recall'] for m in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width, accuracy_scores, width, label='Accuracy', alpha=0.8)\n",
    "bars2 = ax.bar(x, precision_scores, width, label='Precision', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, recall_scores, width, label='Recall', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Models', fontsize=12)\n",
    "ax.set_ylabel('Scores', fontsize=12)\n",
    "ax.set_title('Traffic Accidents: Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim([0.75, 0.90])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('charts/traffic_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices for Traffic Accidents dataset\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (model_name, results) in enumerate(traf_results.items()):\n",
    "    cm = confusion_matrix(traf_target_test, results['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], \n",
    "                cbar_kws={'label': 'Count'})\n",
    "    axes[idx].set_title(f'{model_name}\\nAccuracy: {results[\"accuracy\"]:.4f}', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted Label', fontsize=10)\n",
    "    axes[idx].set_ylabel('True Label', fontsize=10)\n",
    "    axes[idx].tick_params(labelsize=8)\n",
    "\n",
    "# Hide the extra subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.suptitle('Traffic Accidents: Confusion Matrices', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('charts/traffic_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_df = pd.read_csv(\"data/Combined_Flights_2022.csv\")\n",
    "flight_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_target = flight_df[\"Cancelled\"]\n",
    "flight_features = flight_df.drop(columns=[\"Cancelled\"])\n",
    "\n",
    "flight_features = flight_features.dropna(axis=1, how=\"all\")\n",
    "\n",
    "flight_features = flight_features.dropna(axis=1, how=\"any\")\n",
    "\n",
    "flight_features = flight_features.dropna(axis=0, how=\"any\")\n",
    "flight_target = flight_target.loc[flight_features.index]\n",
    "\n",
    "flight_features = flight_features.select_dtypes(include=[\"number\"])\n",
    "\n",
    "print(\"Final target distribution:\")\n",
    "print(flight_target.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200000  # smaller because data set is huge, models take too long\n",
    "\n",
    "flight_features_sampled = flight_features.sample(n=sample_size, random_state=42)\n",
    "flight_target_sampled = flight_target.loc[flight_features_sampled.index]\n",
    "\n",
    "print(\"\\nSampled features shape:\", flight_features_sampled.shape)\n",
    "print(\"Sampled target distribution:\")\n",
    "print(flight_target_sampled.value_counts(dropna=False))\n",
    "\n",
    "flight_features_train, flight_features_test, flight_target_train, flight_target_test = train_test_split(\n",
    "    flight_features_sampled,\n",
    "    flight_target_sampled,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=flight_target_sampled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all flight results\n",
    "flight_results = {}\n",
    "\n",
    "flight_nb = GaussianNB()\n",
    "acc, prec, rec, preds = evaluate_model(\n",
    "    \"Naive Bayes (Flights)\",\n",
    "    flight_nb,\n",
    "    flight_features_train,\n",
    "    flight_target_train,\n",
    "    flight_features_test,\n",
    "    flight_target_test\n",
    ")\n",
    "flight_results['Naive Bayes'] = {'accuracy': acc, 'precision': prec, 'recall': rec, 'predictions': preds}\n",
    "\n",
    "flight_lr = LogisticRegression(max_iter=2000)\n",
    "acc, prec, rec, preds = evaluate_model(\n",
    "    \"Logistic Regression (Flights)\",\n",
    "    flight_lr,\n",
    "    flight_features_train,\n",
    "    flight_target_train,\n",
    "    flight_features_test,\n",
    "    flight_target_test\n",
    ")\n",
    "flight_results['Logistic Regression'] = {'accuracy': acc, 'precision': prec, 'recall': rec, 'predictions': preds}\n",
    "\n",
    "flight_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "acc, prec, rec, preds = evaluate_model(\n",
    "    \"KNN (Flights, k=5)\",\n",
    "    flight_knn,\n",
    "    flight_features_train,\n",
    "    flight_target_train,\n",
    "    flight_features_test,\n",
    "    flight_target_test\n",
    ")\n",
    "flight_results['KNN'] = {'accuracy': acc, 'precision': prec, 'recall': rec, 'predictions': preds}\n",
    "\n",
    "flight_dt = DecisionTreeClassifier(random_state=42)\n",
    "acc, prec, rec, preds = evaluate_model(\n",
    "    \"Decision Tree (Flights)\",\n",
    "    flight_dt,\n",
    "    flight_features_train,\n",
    "    flight_target_train,\n",
    "    flight_features_test,\n",
    "    flight_target_test\n",
    ")\n",
    "flight_results['Decision Tree'] = {'accuracy': acc, 'precision': prec, 'recall': rec, 'predictions': preds}\n",
    "\n",
    "flight_mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=300, random_state=42)\n",
    "acc, prec, rec, preds = evaluate_model(\n",
    "    \"MLP (Flights, 50 hidden units)\",\n",
    "    flight_mlp,\n",
    "    flight_features_train,\n",
    "    flight_target_train,\n",
    "    flight_features_test,\n",
    "    flight_target_test\n",
    ")\n",
    "flight_results['MLP'] = {'accuracy': acc, 'precision': prec, 'recall': rec, 'predictions': preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison for Flights dataset\n",
    "models = list(flight_results.keys())\n",
    "accuracy_scores = [flight_results[m]['accuracy'] for m in models]\n",
    "precision_scores = [flight_results[m]['precision'] for m in models]\n",
    "recall_scores = [flight_results[m]['recall'] for m in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width, accuracy_scores, width, label='Accuracy', alpha=0.8)\n",
    "bars2 = ax.bar(x, precision_scores, width, label='Precision', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, recall_scores, width, label='Recall', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Models', fontsize=12)\n",
    "ax.set_ylabel('Scores', fontsize=12)\n",
    "ax.set_title('Flight Cancellations: Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim([0.93, 0.98])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('charts/flights_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices for Flights dataset\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (model_name, results) in enumerate(flight_results.items()):\n",
    "    cm = confusion_matrix(flight_target_test, results['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[idx], \n",
    "                cbar_kws={'label': 'Count'})\n",
    "    axes[idx].set_title(f'{model_name}\\nAccuracy: {results[\"accuracy\"]:.4f}', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted Label', fontsize=10)\n",
    "    axes[idx].set_ylabel('True Label', fontsize=10)\n",
    "    axes[idx].tick_params(labelsize=8)\n",
    "\n",
    "# Hide the extra subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.suptitle('Flight Cancellations: Confusion Matrices', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('charts/flights_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
